# Snowflake Connection Credentials
# Copy this file to .env and fill in your actual credentials

# Account identifier (e.g., xy12345.us-east-1 or xy12345)
SNOWFLAKE_ACCOUNT=NQHYCCK-OH54539

# User credentials
SNOWFLAKE_USER=your_username

# Password authentication
# SNOWFLAKE_PASSWORD=your_password

# Database and schema context (REQUIRED - no defaults)
SNOWFLAKE_DATABASE=LVDMS
SNOWFLAKE_SCHEMA=LVDMS

# Warehouse (optional - only needed if you want to specify a warehouse)
# Leave this commented out if you don't have a warehouse or want to use default
# SNOWFLAKE_WAREHOUSE=COMPUTE_WH

# Role (optional)
SNOWFLAKE_ROLE=your_role

# Region (optional - for Snowpark, if your account requires explicit region)
# SNOWFLAKE_REGION=us-east-1
# ==============================================================================
# DATABRICKS CONNECTION (OAuth M2M - Service Principal)
# ==============================================================================

# Databricks workspace URL
DATABRICKS_HOST=https://your-workspace.cloud.databricks.com

# OAuth Machine-to-Machine authentication
DATABRICKS_CLIENT_ID=your_client_id
DATABRICKS_CLIENT_SECRET=your_client_secret

# ==============================================================================
# UNITY CATALOG CONFIGURATION (REQUIRED - no defaults in code)
# ==============================================================================

# Unity Catalog name
UC_CATALOG=qubika_partner_solutions

# Unity Catalog schema
UC_SCHEMA=migration_accelerator

# Unity Catalog volume for raw artifacts
UC_RAW_VOLUME=snowflake_artifacts_raw

# ==============================================================================
# SECRETS CONFIGURATION
# ==============================================================================

# Databricks secrets scope name
SECRETS_SCOPE=migration-accelerator

# ==============================================================================
# LLM CONFIGURATION
# ==============================================================================

# Databricks Model Serving endpoint for translation
DBX_ENDPOINT=databricks-llama-4-maverick

# LLM parameters (optional)
# DDL_TEMPERATURE=0.1
# DDL_MAX_TOKENS=2000

# ==============================================================================
# PROCESSING CONFIGURATION
# ==============================================================================

# Batch size for artifact processing
DDL_BATCH_SIZE=8

# Output format: sql, json, or combined
DDL_OUTPUT_FORMAT=sql

# Output directory (REQUIRED - set to your Volume path)
DDL_OUTPUT_DIR=/Volumes/qubika_partner_solutions/migration_accelerator/outputs

# Local DBFS mount for local development
LOCAL_DBFS_MOUNT=./ddl_output

# ==============================================================================
# OBSERVABILITY (optional)
# ==============================================================================

# LangSmith tracing
LANGSMITH_TRACING=true
LANGSMITH_PROJECT=databricks-migration-accelerator
# LANGSMITH_ENDPOINT=https://api.smith.langchain.com
# LANGSMITH_API_KEY=your_langsmith_api_key

# Logging level
LOG_LEVEL=INFO
DDL_VERBOSE_LOGGING=true
DDL_DEBUG=false

# ==============================================================================
# DATABRICKS JOB EXECUTOR (optional)
# ==============================================================================

# Job ID to execute via the Job Executor UI
# DATABRICKS_JOB_ID=123456
