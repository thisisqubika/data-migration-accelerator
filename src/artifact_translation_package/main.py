#!/usr/bin/env python3
"""
Main entry point for the data migration accelerator.

This is the primary entry point for processing JSON files containing artifact definitions.
It determines artifact type from filename and processes them in batches through the translation graph.

Can be used as a CLI tool or imported as a module for programmatic usage.
"""

import sys
import os
import argparse
import json
from datetime import datetime
from typing import List, Dict, Any

from artifact_translation_package.graph_builder import build_translation_graph
from artifact_translation_package.utils.file_processor import process_files, create_batches_from_file
from artifact_translation_package.nodes.aggregator import aggregate_translations
from artifact_translation_package.utils.types import TranslationResult


def process_single_file(
    filepath: str,
    batch_size: int = 10,
    context: Dict[str, Any] = None,
    results_dir: str = None
) -> Dict[str, Any]:
    """
    Process a single file and return translation results.
    
    Args:
        filepath: Path to the DDL file
        batch_size: Number of DDL statements per batch
        context: Optional context dictionary
        results_dir: Timestamped results directory for outputs
        
    Returns:
        Translation results dictionary
    """
    print(f"Processing file: {filepath}")
    print(f"Batch size: {batch_size}")
    print("-" * 50)
    
    if context is None:
        context = {}
    if results_dir:
        context["results_dir"] = results_dir
    
    batches = create_batches_from_file(filepath, batch_size, context)
    
    print(f"Created {len(batches)} batch(es) from file")
    print(f"Total artifacts: {sum(len(batch.items) for batch in batches)}")
    print()
    
    graph = build_translation_graph()
    
    results = []
    for i, batch in enumerate(batches):
        print(f"Processing batch {i + 1}/{len(batches)} ({len(batch.items)} items)...")
        result = graph.run(batch)
        results.append(result)
        print(f"Batch {i + 1} completed")
        print()
    
    if len(results) == 1:
        return results[0]
    
    translation_results = []
    for i, (result, batch) in enumerate(zip(results, batches)):
        translation_result = TranslationResult(
            artifact_type=batch.artifact_type,
            results=result.get(batch.artifact_type, []),
            errors=result.get("metadata", {}).get("errors", []),
            metadata={**result.get("metadata", {}), "batch_index": i}
        )
        translation_results.append(translation_result)
    
    final_result = aggregate_translations(*translation_results)
    return final_result


def process_multiple_files(
    filepaths: List[str],
    batch_size: int = 10,
    context: Dict[str, Any] = None,
    results_dir: str = None
) -> Dict[str, Any]:
    """
    Process multiple files and aggregate results.
    
    Args:
        filepaths: List of file paths to process
        batch_size: Number of DDL statements per batch
        context: Optional context dictionary
        results_dir: Timestamped results directory for outputs
        
    Returns:
        Aggregated translation results
    """
    print(f"Processing {len(filepaths)} file(s)")
    print(f"Batch size: {batch_size}")
    print("=" * 50)
    print()
    
    if context is None:
        context = {}
    if results_dir:
        context["results_dir"] = results_dir
    
    graph = build_translation_graph()
    all_batches = process_files(filepaths, batch_size, context)
    
    print(f"Total batches created: {len(all_batches)}")
    print()
    
    return graph.run_batches(all_batches)


def get_output_directory() -> str:
    """
    Get the output directory path inside translation_graph folder.
    
    Returns:
        Path to output directory
    """
    current_dir = os.path.dirname(os.path.abspath(__file__))
    output_dir = os.path.join(current_dir, "output")
    os.makedirs(output_dir, exist_ok=True)
    return output_dir


def format_sql_content(artifact_type: str, sql_statements: List[str]) -> str:
    """
    Format SQL statements for a specific artifact type.
    
    Args:
        artifact_type: Type of artifact (e.g., 'tables', 'views')
        sql_statements: List of SQL statements
        
    Returns:
        Formatted SQL content string
    """
    artifact_type_upper = artifact_type.upper()
    content = f"-- {artifact_type_upper} DDL - Generated by Translation Graph\n"
    content += f"-- Generated at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
    content += f"-- Total statements: {len(sql_statements)}\n\n"
    
    for i, sql in enumerate(sql_statements, 1):
        cleaned_sql = sql.strip()
        if cleaned_sql.startswith("```sql"):
            cleaned_sql = cleaned_sql[6:].strip()
        if cleaned_sql.startswith("```"):
            cleaned_sql = cleaned_sql[3:].strip()
        if cleaned_sql.endswith("```"):
            cleaned_sql = cleaned_sql[:-3].strip()
        
        content += f"-- Statement {i}\n"
        content += cleaned_sql
        if not cleaned_sql.endswith(";"):
            content += ";"
        content += "\n\n"
    
    return content


def save_sql_files(result: Dict[str, Any], results_dir: str):
    """
    Save SQL results as separate SQL files for each artifact type.
    
    Args:
        result: Translation results dictionary
        results_dir: Timestamped results directory where SQL files will be saved
    """
    sql_dir = os.path.join(results_dir, "sql_files")
    os.makedirs(sql_dir, exist_ok=True)
    
    artifact_types = [
        "databases", "schemas", "tables", "views", "stages", "external_locations",
        "streams", "pipes", "roles", "grants", "tags", "comments",
        "masking_policies", "udfs", "procedures", "file_formats"
    ]
    
    total_sql_files = 0
    total_sql_statements = 0
    
    for artifact_type in artifact_types:
        if artifact_type in result:
            sql_statements = result[artifact_type]
            if sql_statements and isinstance(sql_statements, list) and len(sql_statements) > 0:
                sql_filename = f"{artifact_type}.sql"
                sql_filepath = os.path.join(sql_dir, sql_filename)
                
                content = format_sql_content(artifact_type, sql_statements)
                with open(sql_filepath, 'w', encoding='utf-8') as f:
                    f.write(content)
                
                total_sql_files += 1
                total_sql_statements += len(sql_statements)
                print(f"  ✓ Saved {len(sql_statements)} {artifact_type} SQL statement(s) to sql_files/{sql_filename}")
    
    if total_sql_files > 0:
        abs_sql_dir = os.path.abspath(sql_dir)
        print(f"\n✓ SQL files saved to: {abs_sql_dir}")
        print(f"  Total SQL files: {total_sql_files}")
        print(f"  Total SQL statements: {total_sql_statements}")


def save_results(result: Dict[str, Any], output_dir: str, custom_output_path: str = None, results_dir: str = None):
    """
    Save translation results to timestamped output directory.
    
    Args:
        result: Translation results dictionary
        output_dir: Base output directory
        custom_output_path: Optional custom output file path (if provided, saves there instead)
        results_dir: Pre-created timestamped results directory (if None, creates new one)
    """
    if results_dir is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_dir = os.path.join(output_dir, f"results_{timestamp}")
        os.makedirs(results_dir, exist_ok=True)
     
    if custom_output_path:
        output_path = custom_output_path
        output_file_dir = os.path.dirname(output_path)
        if output_file_dir and not os.path.exists(output_file_dir):
            os.makedirs(output_file_dir, exist_ok=True)
    else:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = os.path.join(results_dir, f"translation_results_{timestamp}.json")
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, indent=2, default=str)
    
    abs_output_path = os.path.abspath(output_path)
    print(f"\n✓ JSON results saved to: {abs_output_path}")
    
    save_sql_files(result, results_dir)


def main():
    """Main entry point for file-based processing."""
    parser = argparse.ArgumentParser(
        description="Process DDL files and translate them using the translation graph"
    )
    parser.add_argument(
        "files",
        nargs="+",
        help="One or more JSON files to process. Artifact type is determined from filename."
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=10,
        help="Number of artifacts per batch (default: 10)"
    )
    parser.add_argument(
        "--output",
        type=str,
        help="Custom output directory path (optional). Default: saves to output/ folder"
    )
    parser.add_argument(
        "--output-format",
        type=str,
        choices=["sql", "json", "combined"],
        default="combined",
        help="Output format: 'sql' (SQL files only), 'json' (JSON only), 'combined' (both). Default: combined"
    )
    
    args = parser.parse_args()
    
    print("Data Migration Accelerator - File Processor")
    print("=" * 50)
    print()
    
    # Determine output directory
    if args.output:
        output_dir = args.output
    else:
        output_dir = get_output_directory()
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_dir = os.path.join(output_dir, f"results_{timestamp}")
    os.makedirs(results_dir, exist_ok=True)
    
    # Set context with results directory for evaluation outputs
    context = {"results_dir": results_dir}
    
    if len(args.files) == 1:
        result = process_single_file(args.files[0], args.batch_size, context=context, results_dir=results_dir)
    else:
        result = process_multiple_files(args.files, args.batch_size, context=context, results_dir=results_dir)
    
    print("Translation Results:")
    print("-" * 50)
    
    for key, value in result.items():
        if key == "metadata":
            print(f"{key}:")
            for meta_key, meta_value in value.items():
                if isinstance(meta_value, dict):
                    print(f"  {meta_key}:")
                    for sub_key, sub_value in meta_value.items():
                        print(f"    {sub_key}: {sub_value}")
                else:
                    print(f"  {meta_key}: {meta_value}")
        elif isinstance(value, list):
            print(f"{key}: {len(value)} items")
            if value and len(value) <= 5:
                for item in value:
                    print(f"  - {item[:100]}..." if len(str(item)) > 100 else f"  - {item}")
        else:
            print(f"{key}: {value}")
    
    # Save results based on output format
    if args.output_format == "json":
        # JSON only
        output_path = os.path.join(results_dir, f"translation_results_{timestamp}.json")
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, default=str)
        abs_output_path = os.path.abspath(output_path)
        print(f"\n✓ JSON results saved to: {abs_output_path}")
    elif args.output_format == "sql":
        # SQL only
        save_sql_files(result, results_dir)
        print(f"\n✓ SQL files saved to: {os.path.abspath(results_dir)}")
    else:  # combined
        # Both JSON and SQL
        output_path = os.path.join(results_dir, f"translation_results_{timestamp}.json")
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, default=str)
        save_sql_files(result, results_dir)
        print(f"\n✓ JSON results saved to: {os.path.abspath(output_path)}")
        print(f"✓ SQL files saved to: {os.path.abspath(results_dir)}")
    
    print("\n✓ Processing completed successfully!")
    print(f"✓ All outputs in: {os.path.abspath(results_dir)}")


if __name__ == "__main__":
    main()

