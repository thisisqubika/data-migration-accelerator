"""
SQL file writing utilities for Migration Accelerator.

This module provides centralized SQL file writing functionality to ensure
consistent behavior across the codebase. It handles both local filesystem
and Databricks DBFS file operations.
"""

import os
import re
from typing import Dict, Any, List, Optional
from datetime import datetime

from artifact_translation_package.utils.sql_cleaner import (
    remove_markdown_code_blocks,
    remove_trailing_semicolons
)
from artifact_translation_package.utils.output_utils import utc_timestamp


# Supported artifact types for SQL file generation
ARTIFACT_TYPES = [
    'tables', 'views', 'schemas', 'databases', 'stages', 'streams',
    'pipes', 'roles', 'grants', 'tags', 'comments',
    'masking_policies', 'udfs', 'procedures', 'external_locations'
]


def create_timestamped_output_dir(base_path: str) -> str:
    """
    Create a timestamped output directory if the base path doesn't already have one.
    
    Args:
        base_path: Base directory path
        
    Returns:
        Path to timestamped output directory
    """
    timestamp_pattern = re.compile(r"^\d{4}-\d{2}-\d{2}T\d{2}-\d{2}-\d{2}Z$")
    base_name = os.path.basename(base_path.rstrip(os.path.sep))
    
    # Check if base path already has a timestamp
    if os.path.exists(base_path) and os.path.isdir(base_path) and not timestamp_pattern.match(base_name):
        # Create timestamped folder under the base
        ts = utc_timestamp()
        output_dir = os.path.join(base_path, ts)
    else:
        output_dir = base_path
    
    # Ensure directory exists
    if not os.path.exists(output_dir):
        os.makedirs(output_dir, exist_ok=True)
    
    return output_dir


def format_sql_header(artifact_type: str, generated_info: str) -> str:
    """
    Format the header for a SQL file.
    
    Args:
        artifact_type: Type of artifact (e.g., 'tables', 'views')
        generated_info: Information about when/where the SQL was generated
        
    Returns:
        Formatted header string
    """
    artifact_type_upper = artifact_type.upper()
    header = f"-- {artifact_type_upper} DDL - Generated by Translation Graph\n"
    header += f"-- Generated: {generated_info}\n\n"
    return header


def format_sql_statement(sql: str, statement_number: int) -> str:
    """
    Format a single SQL statement for file writing.
    
    Args:
        sql: SQL statement to format
        statement_number: Statement number for comment header
        
    Returns:
        Formatted SQL string with comment header and semicolon
    """
    cleaned = remove_markdown_code_blocks(sql.strip())
    cleaned = remove_trailing_semicolons(cleaned)
    
    formatted = f"-- Statement {statement_number}\n"
    formatted += cleaned
    formatted += ";\n\n"
    
    return formatted


def write_sql_file(
    sql_statements: List[str],
    filepath: str,
    artifact_type: str,
    generated_info: str
) -> None:
    """
    Write SQL statements to a file.
    
    Args:
        sql_statements: List of SQL statements to write
        filepath: Path to the output file
        artifact_type: Type of artifact for header
        generated_info: Information for header
    """
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(format_sql_header(artifact_type, generated_info))
        
        for i, sql in enumerate(sql_statements, 1):
            f.write(format_sql_statement(sql, i))


def save_sql_files_local(
    result: Dict[str, Any],
    output_base_path: str,
    logger
) -> Dict[str, int]:
    """
    Save SQL results as separate SQL files for each artifact type (local filesystem).
    
    Args:
        result: Translation results dictionary
        output_base_path: Base path where SQL files will be saved
        logger: Logger instance for reporting
        
    Returns:
        Dictionary with 'total_files' and 'total_statements' counts
    """
    output_dir = create_timestamped_output_dir(output_base_path)
    
    total_sql_files = 0
    total_sql_statements = 0
    
    for artifact_type in ARTIFACT_TYPES:
        if artifact_type in result and result[artifact_type]:
            sql_statements = result[artifact_type]
            if sql_statements:
                sql_filename = f"{artifact_type}.sql"
                sql_filepath = os.path.join(output_dir, sql_filename)
                
                write_sql_file(
                    sql_statements,
                    sql_filepath,
                    artifact_type,
                    os.path.basename(output_base_path)
                )
                
                total_sql_files += 1
                total_sql_statements += len(sql_statements)
                logger.info(
                    "Saved SQL statements",
                    {"artifact_type": artifact_type, "file": sql_filename, "statements": len(sql_statements)}
                )
    
    logger.info(
        "SQL files saved",
        {"path": output_dir, "total_files": total_sql_files, "total_statements": total_sql_statements}
    )
    
    return {"total_files": total_sql_files, "total_statements": total_sql_statements}


def save_sql_files_dbutils(
    result: Dict[str, Any],
    output_base_path: str,
    logger
) -> Dict[str, int]:
    """
    Save SQL results as separate SQL files using dbutils (Databricks DBFS).
    
    Args:
        result: Translation results dictionary
        output_base_path: Base path where SQL files will be saved (dbfs:/ paths)
        logger: Logger instance for reporting
        
    Returns:
        Dictionary with 'total_files' and 'total_statements' counts
    """
    try:
        import dbutils
    except ImportError:
        raise ImportError("dbutils not available. Use save_sql_files_local() instead or run in Databricks environment.")
    
    # Ensure output_base_path ends with a directory separator
    if not output_base_path.endswith('/'):
        output_base_path += '/'
    
    # If caller passed a base directory without timestamp, append a timestamped subdirectory
    timestamp_pattern = re.compile(r"^\d{4}-\d{2}-\d{2}T\d{2}-\d{2}-\d{2}Z$")
    segments = [s for s in output_base_path.split('/') if s]
    last_seg = segments[-1] if segments else ''
    
    if not timestamp_pattern.match(last_seg):
        ts = utc_timestamp()
        output_base_path = output_base_path + ts + '/'
    
    total_sql_files = 0
    total_sql_statements = 0
    
    for artifact_type in ARTIFACT_TYPES:
        if artifact_type in result and result[artifact_type]:
            sql_statements = result[artifact_type]
            if sql_statements:
                sql_filename = f"{artifact_type}.sql"
                sql_filepath = f"{output_base_path}{sql_filename}"
                
                # Build SQL content as string for dbutils
                sql_content = format_sql_header(artifact_type, output_base_path)
                
                for i, sql in enumerate(sql_statements, 1):
                    sql_content += format_sql_statement(sql, i)
                
                dbutils.fs.put(sql_filepath, sql_content)
                
                total_sql_files += 1
                total_sql_statements += len(sql_statements)
                logger.info(
                    "Saved SQL statements to dbfs",
                    {"artifact_type": artifact_type, "file": sql_filename, "statements": len(sql_statements)}
                )
    
    logger.info(
        "SQL files saved to dbfs",
        {"path": output_base_path, "total_files": total_sql_files, "total_statements": total_sql_statements}
    )
    
    return {"total_files": total_sql_files, "total_statements": total_sql_statements}


def save_sql_files(
    result: Dict[str, Any],
    output_base_path: str,
    use_dbutils: bool = False,
    logger=None
) -> Dict[str, int]:
    """
    Save SQL results as separate SQL files for each artifact type.
    
    Automatically detects whether to use dbutils or local filesystem based on
    the use_dbutils parameter.
    
    Args:
        result: Translation results dictionary
        output_base_path: Base path where SQL files will be saved
        use_dbutils: If True, use dbutils for Databricks DBFS; otherwise use local filesystem
        logger: Logger instance for reporting (required)
        
    Returns:
        Dictionary with 'total_files' and 'total_statements' counts
    """
    if logger is None:
        raise ValueError("logger parameter is required")
    
    if use_dbutils:
        return save_sql_files_dbutils(result, output_base_path, logger)
    else:
        return save_sql_files_local(result, output_base_path, logger)