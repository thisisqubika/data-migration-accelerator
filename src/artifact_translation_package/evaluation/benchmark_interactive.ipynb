{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# SQL Translation Model Benchmark - Interactive\n",
                "\n",
                "Simple notebook interface to run benchmarks using the `run_local_benchmark.py` script.\n",
                "\n",
                "## What This Does\n",
                "\n",
                "- ğŸ¤– Compares SQL translation models (Llama 4 Maverick vs Gemini 2.5 Flash)\n",
                "- âš–ï¸ Uses LLM-as-judge to evaluate Databricks SQL compliance\n",
                "- ğŸ“Š Tracks results in Databricks MLflow\n",
                "- ğŸ“ˆ Shows comparison charts"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Configuration\n",
                "\n",
                "Edit these settings to customize your benchmark:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "# CONFIGURATION - Customize these settings\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "\n",
                "# Models to benchmark (for SQL translation)\n",
                "TRANSLATION_MODELS = [\n",
                "    \"databricks-llama-4-maverick\",\n",
                "    \"databricks-gemini-2-5-flash\",\n",
                "]\n",
                "\n",
                "# Judge model (evaluates SQL compliance)\n",
                "JUDGE_MODEL = \"databricks-llama-4-maverick\"\n",
                "\n",
                "# Artifact type to benchmark\n",
                "ARTIFACT_TYPE = \"tables\"  # Options: tables, views, procedures\n",
                "\n",
                "# Batch size for processing\n",
                "BATCH_SIZE = 5\n",
                "\n",
                "# Optional: Custom dataset path (None = use built-in examples)\n",
                "DATASET_PATH = None  # e.g., \"../examples/tables.json\"\n",
                "\n",
                "print(\"ğŸ“‹ Configuration:\")\n",
                "print(f\"  Translation models: {TRANSLATION_MODELS}\")\n",
                "print(f\"  Judge model: {JUDGE_MODEL}\")\n",
                "print(f\"  Artifact type: {ARTIFACT_TYPE}\")\n",
                "print(f\"  Batch size: {BATCH_SIZE}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Run Benchmark\n",
                "\n",
                "Execute the benchmark with your configuration:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "from pathlib import Path\n",
                "\n",
                "# Add project root to path\n",
                "project_root = Path.cwd()\n",
                "while not (project_root / \"run_local_benchmark.py\").exists() and project_root.parent != project_root:\n",
                "    project_root = project_root.parent\n",
                "\n",
                "if not (project_root / \"run_local_benchmark.py\").exists():\n",
                "    print(\"âŒ Could not find run_local_benchmark.py\")\n",
                "    print(f\"   Searched from: {Path.cwd()}\")\n",
                "    print(f\"   Ended at: {project_root}\")\n",
                "else:\n",
                "    print(f\"âœ… Found project root: {project_root}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Build command arguments\n",
                "import subprocess\n",
                "\n",
                "cmd = [\n",
                "    \"python3\",\n",
                "    str(project_root / \"run_local_benchmark.py\"),\n",
                "    \"--artifact-type\", ARTIFACT_TYPE,\n",
                "    \"--batch-size\", str(BATCH_SIZE),\n",
                "    \"--judge-endpoint\", JUDGE_MODEL,\n",
                "    \"--models\"\n",
                "] + TRANSLATION_MODELS\n",
                "\n",
                "if DATASET_PATH:\n",
                "    cmd.extend([\"--dataset-source\", str(DATASET_PATH)])\n",
                "\n",
                "print(\"ğŸš€ Running benchmark...\")\n",
                "print(f\"\\nCommand: {' '.join(cmd)}\")\n",
                "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
                "\n",
                "# Run the benchmark\n",
                "result = subprocess.run(cmd, capture_output=False, text=True)\n",
                "\n",
                "if result.returncode == 0:\n",
                "    print(\"\\n\" + \"=\"*70)\n",
                "    print(\"âœ… Benchmark completed successfully!\")\n",
                "    print(\"=\"*70)\n",
                "else:\n",
                "    print(\"\\n\" + \"=\"*70)\n",
                "    print(f\"âŒ Benchmark failed with exit code: {result.returncode}\")\n",
                "    print(\"=\"*70)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. View Results in Databricks MLflow\n",
                "\n",
                "After running the benchmark:\n",
                "\n",
                "1. **Open Databricks workspace**\n",
                "2. **Navigate to**: Machine Learning â†’ Experiments\n",
                "3. **Find your user folder**: `/Users/your.email@domain.com/`\n",
                "4. **Open experiment**: `sql-translation-benchmark`\n",
                "5. **Compare runs**: Select multiple runs and click \"Compare\"\n",
                "\n",
                "### Tips\n",
                "\n",
                "- **Grouped bar charts**: Select 2+ runs to see side-by-side comparisons\n",
                "- **Metrics to compare**: `avg_compliance_score`, `syntax_valid_pct`, `best_practices_pct`\n",
                "- **LLM traces**: View in each run's \"Traces\" tab for debugging"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.13.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
